{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0M8fEBrhFnt"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Innocent Mamvura (innocent.mamvura@wits.ac.za)<br>\n",
    "Data Scientist <br>\n",
    "[Wits University](https://www.wits.ac.za//)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2fp1X3NAqwd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAWG_-xnbmk0"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6S5nCuaOl9Pm"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "An [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANN) was conceived as a model which would learn in a manner similar to the brain.\n",
    "\n",
    "It's a mistake to take this analogy too literally though.\n",
    "\n",
    "> However, modern neural network research is guided by many mathematical and engineering disciplines, and **the goal of neural networks is not to perfectly model the brain**. It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization, occasionally **drawing some insights from what we know about the brain, rather than as models of brain function**.\n",
    ">\n",
    "> &mdash; Goodfellow, Bengio & Courville: *Deep Learning* (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zb7NnhIlTPR0"
   },
   "source": [
    "### Neurons\n",
    "\n",
    "The neurons in a neural network were originally intended to model the neurons in the brain. It's a very rough, approximate model. The basic idea is this:\n",
    "\n",
    "- a neuron receives stimuli (\"inputs\") from its dendrites;\n",
    "- these stimuli are aggregated and if they exceed some threshold then they cause the neuron to \"fire\";\n",
    "- when a neuron fires it sends output along its axon.\n",
    "\n",
    "The original idea for this model was published in \"[A logical calculus of the ideas immanent in nervous activity](https://doi.org/10.1007/BF02478259)\" (McCulloch & Pitts, The Bulletin of Mathematical Biophysics, 5(4), 115â€“133, 1943).\n",
    "\n",
    "![](https://github.com/datawookie/useful-images/raw/master/neuron-wikipedia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZRWxhnjz92C"
   },
   "source": [
    "### Neuron Model\n",
    "\n",
    "This is what the mathematical model of a neuron looks like:\n",
    "\n",
    "$$\n",
    "g\\left(\\sum_i w_i \\cdot x_i + b\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x_i$ are the input signals;\n",
    "- $w_i$ are the weights attached to each of the input channels;\n",
    "- $b$ is a constant bias value; and\n",
    "- $g()$ is a non-linear activation function.\n",
    "\n",
    "The process for calculating the output from the neuron model is:\n",
    "\n",
    "- multiply the inputs by the corresponding weights and sum the products;\n",
    "- add the bias; and\n",
    "- apply an activation function.\n",
    "\n",
    "This calculation consists of two components:\n",
    "\n",
    "- *linear component* &mdash; multiplication and addition; and\n",
    "- *non-linear component* &mdash; the activation function.\n",
    "\n",
    "This model is illustrated below.\n",
    "\n",
    "![](https://raw.githack.com/datawookie/useful-images/master/neuron-artificial.svg)\n",
    "\n",
    "The inputs and weights for a single neuron are often represented by vectors, in which case the operation is given as\n",
    "\n",
    "$$\n",
    "g\\left(w \\cdot x +b\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeNB_Rw5Q6jY"
   },
   "source": [
    "### Network\n",
    "\n",
    "A single neuron in isolation doesn't do an awful lot. However, when you take a bunch of neurons and connect them in a network, suddenly you have something which is rather powerful.\n",
    "\n",
    "A [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) is a network consisting of three layers:\n",
    "\n",
    "- input layer\n",
    "- hidden layer and\n",
    "- output layer.\n",
    "\n",
    "<!-- Image created with http://alexlenail.me/NN-SVG/. Can do CNN too. -->\n",
    "\n",
    "![](https://github.com/datawookie/useful-images/raw/master/neural-network.png)\n",
    "\n",
    "The above network is \"dense\" or \"fully connected\": every node in a layer is linked to each node in the next layer.\n",
    "\n",
    "The input layer (left) consists of the features being fed into the model. The output layer (right) is the prediction being generated by the model.\n",
    "\n",
    "The maths remains essentially the same (although there's now a lot more of it!). Now, rather than a vector or weights, there's a *weight matrix* associated with each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNxqsaBy51-r"
   },
   "source": [
    "### Universal Approximation Theorem\n",
    "\n",
    "The [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that a feed-forward network with a single hidden layer with a finite number of nodes *can* approximate essentially any function. However, it does not say whether it's feasible for such a network to be trained in practice.\n",
    "\n",
    "> In summary, a **feedforward network with a single layer is sufficient to represent\n",
    "any function**, but the layer may be infeasibly large and may fail to learn and generalize correctly. In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error.\n",
    ">\n",
    "> &mdash; Goodfellow, Bengio & Courville: *Deep Learning* (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMUphBTLYM36"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "A key component of how an ANN works is the activation function. This determines whether or not a given neuron will respond to its inputs.\n",
    "\n",
    "For a multi-layer network it's often the case that a different activation function is used for each layer in the network.\n",
    "\n",
    "There are a wide range of activation functions to choose from. The most common ones are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFjJuG4XsjML"
   },
   "source": [
    "### Linear\n",
    "\n",
    "A linear activation function simply maps its input to its output. It's just the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1569772100900,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "9B77C2qetS_M",
    "outputId": "deba27cb-4c1a-4b16-f828-2732e90740b1"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = x\n",
    "\n",
    "sns.lineplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra0s0PXP6Yuy"
   },
   "source": [
    "We'll ultimately be building networks using Keras, so this is a good time to see what activations in Keras look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1569772006474,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "wWXGVleYsoxx",
    "outputId": "4fb97077-b1b0-4928-e2d8-5a2158f27148"
   },
   "outputs": [],
   "source": [
    "keras.activations.linear(tf.constant([-5, 0, 5], dtype=tf.float32)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WujuVHATZ2ft"
   },
   "source": [
    "### ReLU\n",
    "\n",
    "The [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (Rectified Linear Unit) function returns the maximum of its argument and zero. ReLU is normally used in hidden layers.\n",
    "\n",
    "Here the \"response\" nature of the activation function is apparent: if the input is less than zero then the output is zero (no response), but if the input is greater than zero then the response is equal to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "J2PG-UaSBCYJ",
    "outputId": "c1085d42-2c5b-4fdf-fe9a-95a95acdf2f1"
   },
   "outputs": [],
   "source": [
    "y = np.maximum(0, x)\n",
    "\n",
    "sns.lineplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2606,
     "status": "ok",
     "timestamp": 1569167583669,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "a3WG155qZ8Xa",
    "outputId": "b58fdea6-a64e-4415-ccaa-e5419285b22d"
   },
   "outputs": [],
   "source": [
    "keras.activations.relu(tf.constant([-5, 0, 5], dtype=tf.float32)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1bUAWvxouuY"
   },
   "source": [
    "### ELU\n",
    "\n",
    "The ELU (Exponential Linear Unit) activation function is like ReLU for positive arguments but exponential for negative arguments:\n",
    "\n",
    "$$\n",
    "f(x)={\\begin{cases}x&{\\text{if }}x>0,\\\\ \\alpha(e^{x}-1)&{\\text{otherwise}}.\\end{cases}}\n",
    "$$\n",
    "\n",
    "It gives a smoother transition at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2042,
     "status": "ok",
     "timestamp": 1569167321762,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "cA0YADMCppW9",
    "outputId": "b444a957-2bd7-4743-e437-4f83bfbed35b"
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "#\n",
    "y = np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "sns.lineplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1569167598734,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "ZnWqF2N_rVAv",
    "outputId": "81ff2e47-59f2-4c6f-d2ba-ec252ec0cc5b"
   },
   "outputs": [],
   "source": [
    "keras.activations.elu(tf.constant([-5, 0, 5], dtype=tf.float32)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ouyr7q7sTpZ"
   },
   "source": [
    "### Sigmoid\n",
    "\n",
    "The [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function is generally used for the output later of a binary (two class) classification problem. This function effectively \"squashes\" it's input to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1694,
     "status": "ok",
     "timestamp": 1569167203791,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "Or3PZ4jwAOkp",
    "outputId": "365d9fc9-3e33-40de-fc9d-2d5f18b6d46a"
   },
   "outputs": [],
   "source": [
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "sns.lineplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "GM8MHZMcZ7pV",
    "outputId": "38ff63c4-0694-4bdf-bd36-e0b895049abd"
   },
   "outputs": [],
   "source": [
    "keras.activations.sigmoid(tf.constant([-5, 0, 5], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7E9OaZF2Vsif"
   },
   "source": [
    "### Hyperbolic Tangent\n",
    "\n",
    "The [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function) (or *tanh*) is similar to the sigmoid, but its output is between -1 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "AeP8QRyuVrim",
    "outputId": "a0ee33bf-d38e-4864-9e58-d13b5d977300"
   },
   "outputs": [],
   "source": [
    "y = np.tanh(x)\n",
    "\n",
    "sns.lineplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1570010836966,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "8skIPCYA74_Y",
    "outputId": "1e35485e-6e77-484f-9805-3158600373c9"
   },
   "outputs": [],
   "source": [
    "keras.activations.tanh(tf.constant([-5, 0, 5], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gtK23sYZ11a"
   },
   "source": [
    "### Softmax\n",
    "\n",
    "The [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function is used in the output layer of classification problems with more than two target classes. The output can be interpreted as probabilities of each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1570010934173,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "T6tG3THTZ9V5",
    "outputId": "9a3917d8-8ba9-4613-d7fb-0e4ec7371de1"
   },
   "outputs": [],
   "source": [
    "keras.activations.softmax(tf.constant([[1, 7, 2]], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIUyOLc68dIl"
   },
   "source": [
    "*Note:* The output from the softmax is not quite as simple as scaling the arguments so that they sum to one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPO-DFqxayzB"
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "A *loss function* (or *cost function*) indicates how well a model fits the data. Smaller values of the loss function are associated with better fitting models. So fitting a model is equivalent to minimising the loss function.\n",
    "\n",
    "These are some commonly used loss functions:\n",
    "\n",
    "- Mean Squared Error (MSE) &mdash; regression;\n",
    "- Mean Absolute Error (MAE) &mdash; regression;\n",
    "- Binary Cross-Entropy &mdash; binary classification and\n",
    "- Categorical Cross-Entropy &mdash; multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e727tZwxHiYk"
   },
   "source": [
    "For many common Machine Learning applications the loss function is *convex*, which means that it's easy to find the minimum value. Deep Learning problems, on the other hand, often have loss functions which are not convex, with many local minima. As a result, finding the global minimum can be much more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1991,
     "status": "ok",
     "timestamp": 1570014317944,
     "user": {
      "displayName": "Andrew Collier",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCvrDr9ggh2JX2LPB-tW5bxoI6EprrLY52Q6BnN=s64",
      "userId": "07683598884238397219"
     },
     "user_tz": -120
    },
    "id": "-CZgU4bRHZJR",
    "outputId": "6464b817-5bd9-4d3e-c673-b63d3f63aabc"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 6))\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "sns.lineplot(x, x**2)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "sns.lineplot(x, x**2 + np.sin(x*10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQxBD0QEe2pr"
   },
   "source": [
    "## Optimisers\n",
    "\n",
    "An optimiser is an algorithm which tries to find a set of parameters which minimise a loss function.\n",
    "\n",
    "> The largest difference between the linear models we have seen so far and neural networks is that the **nonlinearity of a neural network causes most interesting loss functions to become non-convex**. This means that **neural networks are usually trained by using iterative, gradient-based optimizers that merely drive the cost function to a very low value**, rather than the linear equation solvers used to train linear regression models or the convex optimization algorithms with global convergence guarantees used to train logistic regression or SVMs. **Convex optimization converges starting from any initial parameters. Stochastic gradient descent applied to non-convex loss functions has no such convergence guarantee, and is sensitive to the values of the initial parameters.** For feedforward neural networks, **it is important to initialize all weights to small random values**. The **biases may be initialized to zero or to small positive values**.\n",
    ">\n",
    "> &mdash; Goodfellow, Bengio & Courville: *Deep Learning* (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgSfylsuN-M3"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The principle of the [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm is simple: just move \"down hill\" (in the opposite direction to the gradient) on the loss function until you get to the bottom.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/560px-Gradient_descent.svg.png\">\n",
    "\n",
    "The *learning rate* determines the size of each step:\n",
    "\n",
    "- small learning rate &mdash; gradual convergence; and\n",
    "- large learning rate &mdash; rapid convergence but less stable and might miss global minimum.\n",
    "\n",
    "This is also known as \"batch\" gradient descent because the entire dataset is used to calculate the gradient.\n",
    "\n",
    "If the loss function is convex (seldom the case for a neural network) then this simple approach works fine. However, if there are local minima this algorithm is likely to get stuck.\n",
    "\n",
    "There are variations on Gradient Descent which make it applicable to non-convex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wN718CuN6yR"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "[Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) is like Gradient Descent but, rather than calculating the actual gradient of the loss function using the entire dataset, it approximates the gradient using a randomly selected subset of the data. As a result there are more, faster iterations.\n",
    "\n",
    "The *momentum* combines updates from previous iterations with the gradient from the current interation. Momentum is an important feature because it allows the optimiser to jump out of local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wa6tUmhmelLZ"
   },
   "source": [
    "There are other optimisation techniques which are significantly more efficient than SGD.\n",
    "\n",
    "![](https://github.com/datawookie/useful-images/raw/master/optimisation-techniques-saddle-point.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCcAMJEimb9G"
   },
   "source": [
    "### Batches and Mini-Batches\n",
    "\n",
    "Strictly speaking, each update in Stochastic Gradient Descent should be based on a single, randomly selected sample from the data.\n",
    "\n",
    "At the opposite extreme, \"batch\" Gradient Descent uses the complete dataset for each update.\n",
    "\n",
    "There is a compromise between these two extremes: train on subsets (or \"mini-batches\") of data. When the model has been trained on all mini-batches it's called an \"epoch\".\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Less time per iteration (more frequent updates).\n",
    "- Less RAM per iteration (and can handle larger data).\n",
    "- Results are more noisy, but this can be good because it helps escape from local minima.\n",
    "\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- More iterations.\n",
    "- How to find \"best\" mini-batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECOwJtjrdl3h"
   },
   "source": [
    "### RMS Propagation\n",
    "\n",
    "The Root Mean Square (RMS) Propagation (or RMSProp) optimiser has an adaptive learning rate *per parameter*. Recent gradients are used to provide a smoothed update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJhlfyqQeNCF"
   },
   "source": [
    "### Adaptive Moment\n",
    "\n",
    "The Adaptive Moment (or Adam) optimiser is similar to RMS Propagation but uses a more sophisticated technique to incorporate gradients from previous iterations. It tends to perform better without tuning to individual problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKzK9J_JZyQP"
   },
   "source": [
    "## Resources\n",
    "\n",
    "- [How does backpropagation work?](https://brohrer.github.io/how_backpropagation_works.html)\n",
    "- [What are the meanings of batch size, mini-batch, iterations and epoch in neural networks?](https://www.quora.com/What-are-the-meanings-of-batch-size-mini-batch-iterations-and-epoch-in-neural-networks)\n",
    "- [What is meant by \"Batch\" in machine learning?](https://www.quora.com/What-is-meant-by-Batch-in-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLmq892Fv6_5"
   },
   "source": [
    "![](https://github.com/datawookie/useful-images/raw/master/banner/banner-lab-tensorflow-keras.png)\n",
    "\n",
    "The lab for this section will involve building a simple Neural Network model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04-neural-networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
